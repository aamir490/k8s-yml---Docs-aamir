
#############################################################################################################

#############################################################################################################
## connect [ k8s client - redhat linux - ec2 instance ] 

$ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
$ helm repo ls
$ helm repo update
$ helm search repo prometheus-community

$ helm template prometheus prometheus-community/prometheus     <---- ( want to know default value of this cahrt {prometheus-community/prometheus} )
$ helm show values prometheus-community/prometheus             <---- ( want to know values file )

  ------> Below are very lenghty file { use mithun file { https://github.com/aamir490/Kubernates-Manifests/blob/master/prometheusvalues.yml }

$ helm show values  prometheus-community/prometheus | less     <---- ( there is no alert rules )
$ helm show values  prometheus-community/prometheus | grep "alerting_rules.yml"
$ helm show values  prometheus-community/prometheus >> test.txt

  -------> create name sapce name "monitoring"

$ kubectl create ns monitoring
$ kubectl get all -n monitoring


$ --> helm install <Release Name> <Chart Name>/<Repo Name> -n <Namespace Name>
$ helm install prometheus prometheus-community/prometheus -n monitoring
$ helm ls -n monitoring
$ kubectl get cm -n monitoring
$ kubectl get all -n monitoring
$ kubectl describe pod prometheus-server-568d75474d-2kxll -n monitoring
$ kubectl get pvc -n monitoring


---  # prometheus values ( https://github.com/aamir490/Kubernates-Manifests/blob/master/prometheusvalues.yml )
alertmanager:
  config:
    global:
      resolve_timeout: 1m
      # slack_api_url: ''

    receivers:
      - name: 'gmail-notifications'
        email_configs:
        - to: devopstrainingbanglore@gmail.com
          from: devopslearningnine@gmail.com # Update your from mail id here
          smarthost: smtp.gmail.com:587
          auth_username: devopslearningnine@gmail.com # Update your from mail id here
          auth_identity: devopslearningnine@gmail.com # Update your from mail id here
          auth_password: XXXXXXXXX  # Update your password here
          send_resolved: true
          headers:
            subject: " Prometheus -  Alert  "
          text: "{{ range .Alerts }} Hi, \n{{ .Annotations.summary }}  \n {{ .Annotations.description }} {{end}} "
        # slack_configs:
        #  - channel: '@you'
        #    send_resolved: true

    route:
      group_wait: 10s
      group_interval: 2m
      receiver: 'gmail-notifications'
      repeat_interval: 2m
serverFiles:
  alerting_rules.yml:
      groups:
      - name: NodeDown
        rules:
        # Alert for any instance that is unreachable for >5 minutes.
        - alert: InstanceDown
          expr: up{job="kubernetes-nodes"} == 0
          for: 2m
          labels:
            severity: page
          annotations:
            host: "{{ $labels.kubernetes_io_hostname }}"
            summary: "Instance down"
            description: "Node {{ $labels.kubernetes_io_hostname  }}has been down for more than 5 minutes."
      - name: low_memory_alert
        rules:
        - alert: LowMemory
          expr: (node_memory_MemAvailable_bytes /  node_memory_MemTotal_bytes) * 100 < 15
          for: 2m
          labels:
            severity: warning
          annotations:
            host: "{{ $labels.kubernetes_node  }}"
            summary: "{{ $labels.kubernetes_node }} Host is low on memory.  Only {{ $value }}% left"
            description: "{{ $labels.kubernetes_node }}  node is low on memory.  Only {{ $value }}% left"
        - alert: KubePersistentVolumeErrors
          expr: kube_persistentvolume_status_phase{job="kubernetes-service-endpoints",phase=~"Failed|Pending"} > 0
          for: 2m
          labels:
            severity: critical
          annotations:
            description: The persistent volume {{ $labels.persistentvolume }} has status {{ $labels.phase }}.
            summary: PersistentVolume is having issues with provisioning.
        - alert: KubePodCrashLooping
          expr: rate(kube_pod_container_status_restarts_total{job="kubernetes-service-endpoints",namespace=~".*"}[5m]) * 60 * 5 > 0
          for: 2m
          labels:
            severity: warning
          annotations:
            description: Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is restarting {{ printf "%.2f" $value }} times / 5 minutes.
            summary: Pod is crash looping.
        - alert: KubePodNotReady
          expr: sum by(namespace, pod) (max by(namespace, pod) (kube_pod_status_phase{job="kubernetes-service-endpoints",namespace=~".*",phase=~"Pending|Unknown"}) * on(namespace, pod)    group_left(owner_kind) topk by(namespace, pod) (1, max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!="Job"}))) > 0
          for: 2m
          labels:
            severity: warning
          annotations:
            description: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 5 minutes.
            summary: Pod has been in a non-ready state for more than 2 minutes.
...



